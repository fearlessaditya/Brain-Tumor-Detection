{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf544b-dd75-411d-a434-72c9bd2e42c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Classes found: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
      "Training samples per class: {'glioma': 1321, 'meningioma': 1339, 'notumor': 1595, 'pituitary': 1457}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2864 | Train Acc: 90.06%\n",
      "Val   Loss: 0.2286 | Val   Acc: 92.60%\n",
      ">>> New best lowest val_loss: 0.228582 at epoch 1\n",
      "Saved checkpoint: outputs\\best_brain_tumor_model.pth\n",
      "Saved full model object: C:\\Users\\ACER\\Desktop\\archive (3)\\trained_brain_tumor_model.h5\n",
      "\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1747 | Train Acc: 94.36%\n",
      "Val   Loss: 0.3274 | Val   Acc: 89.86%\n",
      "No improvement in val_loss. EarlyStopping counter: 1/3\n",
      "\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1241 | Train Acc: 95.66%\n",
      "Val   Loss: 0.2484 | Val   Acc: 91.84%\n",
      "No improvement in val_loss. EarlyStopping counter: 2/3\n",
      "\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1109 | Train Acc: 96.29%\n",
      "Val   Loss: 0.2357 | Val   Acc: 92.30%\n",
      "No improvement in val_loss. EarlyStopping counter: 3/3\n",
      "Early stopping triggered at epoch 4. Best epoch: 1\n",
      "Saved final model object to: C:\\Users\\ACER\\Desktop\\archive (3)\\final_brain_tumor_model.h5\n",
      "Training history saved to outputs\\training_history.png\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      glioma     0.9961    0.8533    0.9192       300\n",
      "  meningioma     0.7972    0.9379    0.8619       306\n",
      "     notumor     0.9463    1.0000    0.9724       405\n",
      "   pituitary     0.9850    0.8733    0.9258       300\n",
      "\n",
      "    accuracy                         0.9230      1311\n",
      "   macro avg     0.9311    0.9161    0.9198      1311\n",
      "weighted avg     0.9317    0.9230    0.9238      1311\n",
      "\n",
      "Confusion matrix saved to outputs\\confusion_matrix.png\n",
      "\n",
      "Training completed. Best val_loss: 0.22858198353783443. Best epoch: 1\n",
      "Checkpoints in outputs and full .h5 models in C:\\Users\\ACER\\Desktop\\archive (3)\n"
     ]
    }
   ],
   "source": [
    "# train_brain_tumor_with_earlystopping.py\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models, datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "TRAIN_DIR = r\"C:\\Users\\ACER\\Desktop\\archive (3)\\Training\"\n",
    "VAL_DIR   = r\"C:\\Users\\ACER\\Desktop\\archive (3)\\Testing\"\n",
    "ARCHIVE_DIR = Path(r\"C:\\Users\\ACER\\Desktop\\archive (3)\")  \n",
    "\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = 224\n",
    "BASE_MODEL_NAME = \"resnet50\"  \n",
    "PRETRAINED = True\n",
    "FREEZE_BACKBONE = False\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CLIP_GRAD_NORM = 1.0\n",
    "NUM_WORKERS = min(8, os.cpu_count() or 4)\n",
    "SEED = 42\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Early stopping / monitoring\n",
    "MONITOR = \"val_loss\"  \n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "MIN_DELTA = 1e-4  \n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def get_transforms(image_size=224):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std =[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(int(image_size*1.14)),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std =[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_transform, val_transform\n",
    "\n",
    "def make_dataloaders(train_dir, val_dir, batch_size, num_workers):\n",
    "    train_tf, val_tf = get_transforms(IMAGE_SIZE)\n",
    "    train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n",
    "    val_ds = datasets.ImageFolder(val_dir, transform=val_tf)\n",
    "\n",
    "    targets = [s[1] for s in train_ds.samples]\n",
    "    class_sample_count = np.array([targets.count(c) for c in range(len(train_ds.classes))])\n",
    "    class_sample_count = np.where(class_sample_count == 0, 1, class_sample_count)\n",
    "    class_weights = 1.0 / class_sample_count\n",
    "    samples_weight = np.array([class_weights[t] for t in targets])\n",
    "    samples_weight = torch.from_numpy(samples_weight).double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    print(f\"Classes found: {train_ds.classes}\")\n",
    "    print(\"Training samples per class:\", dict(zip(train_ds.classes, class_sample_count)))\n",
    "    return train_loader, val_loader, train_ds.classes\n",
    "\n",
    "def create_model(num_classes=4, backbone=\"resnet50\", pretrained=True, freeze_backbone=False):\n",
    "    if backbone == \"resnet50\":\n",
    "        model = models.resnet50(pretrained=pretrained)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "    elif backbone == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=pretrained)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "    elif backbone == \"efficientnet_b0\":\n",
    "        model = models.efficientnet_b0(pretrained=pretrained)\n",
    "        in_features = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"fc\" in name or \"classifier\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if monitored metric doesn't improve after a given patience.\n",
    "    When improvement happens it saves a checkpoint (.pth) and the full model (.h5) to disk.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, min_delta=1e-4, mode=\"min\",\n",
    "                 checkpoint_dir=OUTPUT_DIR, archive_dir=ARCHIVE_DIR,\n",
    "                 checkpoint_name=\"best_brain_tumor_model.pth\",\n",
    "                 fullmodel_name=\"trained_brain_tumor_model.h5\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.archive_dir = Path(archive_dir)\n",
    "        self.checkpoint_path = self.checkpoint_dir / checkpoint_name\n",
    "        self.fullmodel_path = self.archive_dir / fullmodel_name\n",
    "        self.counter = 0\n",
    "        self.best_epoch = None\n",
    "        if mode == \"min\":\n",
    "            self.best_score = math.inf\n",
    "        elif mode == \"max\":\n",
    "            self.best_score = -math.inf\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'min' or 'max'\")\n",
    "\n",
    "    def _is_improvement(self, current):\n",
    "        if self.mode == \"min\":\n",
    "            return (self.best_score - current) > self.min_delta\n",
    "        else:\n",
    "            return (current - self.best_score) > self.min_delta\n",
    "\n",
    "    def step(self, current, model, optimizer, epoch, classes):\n",
    "        \"\"\"\n",
    "        Call after each validation. Returns True if training should stop.\n",
    "        If improvement -> saves checkpoint and resets counter.\n",
    "        \"\"\"\n",
    "        improved = False\n",
    "        if self._is_improvement(current):\n",
    "            improved = True\n",
    "            self.best_score = current\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            # save checkpoint (.pth)\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"monitor_value\": current,\n",
    "                \"classes\": classes\n",
    "            }, str(self.checkpoint_path))\n",
    "            # save full model object (.h5) to archive folder\n",
    "            torch.save(model, str(self.fullmodel_path))\n",
    "            print(f\">>> New best { 'lowest' if self.mode=='min' else 'highest' } {MONITOR}: {current:.6f} at epoch {epoch+1}\")\n",
    "            print(f\"Saved checkpoint: {self.checkpoint_path}\")\n",
    "            print(f\"Saved full model object: {self.fullmodel_path}\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement in {MONITOR}. EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for inputs, labels in pbar:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        pbar.set_postfix(loss=f\"{running_loss/total:.4f}\", acc=f\"{100.*correct/total:.2f}%\")\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for inputs, labels in pbar:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{running_loss/total:.4f}\", acc=f\"{100.*correct/total:.2f}%\")\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "def plot_history(train_losses, val_losses, train_accs, val_accs, out_path):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='train_loss')\n",
    "    plt.plot(val_losses, label='val_loss')\n",
    "    plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(); plt.title('Loss')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accs, label='train_acc')\n",
    "    plt.plot(val_accs, label='val_acc')\n",
    "    plt.xlabel('epoch'); plt.ylabel('accuracy (%)'); plt.legend(); plt.title('Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, classes, out_path):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    train_loader, val_loader, classes = make_dataloaders(TRAIN_DIR, VAL_DIR, BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "    model = create_model(num_classes=NUM_CLASSES, backbone=BASE_MODEL_NAME, pretrained=PRETRAINED,\n",
    "                         freeze_backbone=FREEZE_BACKBONE)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    # Early stopping instance\n",
    "    es_mode = \"min\" if MONITOR == \"val_loss\" else \"max\"\n",
    "    early_stopper = EarlyStopping(patience=EARLY_STOPPING_PATIENCE,\n",
    "                                  min_delta=MIN_DELTA,\n",
    "                                  mode=es_mode,\n",
    "                                  checkpoint_dir=OUTPUT_DIR,\n",
    "                                  archive_dir=ARCHIVE_DIR)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    last_val_preds, last_val_labels = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch [{epoch+1}/{EPOCHS}]\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, DEVICE)\n",
    "        val_loss, val_acc, val_preds, val_labels = validate_epoch(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        # choose metric for monitoring\n",
    "        monitored_value = val_loss if MONITOR == \"val_loss\" else val_acc\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save last checkpoint (.pth)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"val_acc\": val_acc,\n",
    "            \"classes\": classes\n",
    "        }, OUTPUT_DIR / \"last_checkpoint.pth\")\n",
    "\n",
    "        # Step early stopping \n",
    "        stop = early_stopper.step(monitored_value, model, optimizer, epoch, classes)\n",
    "\n",
    "        # also keep track of best val_acc to print at the end\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        last_val_preds, last_val_labels = val_preds, val_labels\n",
    "\n",
    "        if stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Best epoch: {early_stopper.best_epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Save final model object (last epoch) to archive folder as well\n",
    "    final_h5_path = ARCHIVE_DIR / \"final_brain_tumor_model.h5\"\n",
    "    torch.save(model, str(final_h5_path))\n",
    "    print(f\"Saved final model object to: {final_h5_path}\")\n",
    "\n",
    "    # Save training history & plots\n",
    "    plot_history(history[\"train_loss\"], history[\"val_loss\"],\n",
    "                 history[\"train_acc\"], history[\"val_acc\"],\n",
    "                 OUTPUT_DIR / \"training_history.png\")\n",
    "    print(\"Training history saved to\", OUTPUT_DIR / \"training_history.png\")\n",
    "\n",
    "    # classification report & confusion matrix on last validation predictions\n",
    "    if last_val_labels:\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(last_val_labels, last_val_preds, target_names=classes, digits=4))\n",
    "        save_confusion_matrix(last_val_labels, last_val_preds, classes, OUTPUT_DIR / \"confusion_matrix.png\")\n",
    "        print(\"Confusion matrix saved to\", OUTPUT_DIR / \"confusion_matrix.png\")\n",
    "    else:\n",
    "        print(\"No validation results to report.\")\n",
    "\n",
    "    print(f\"\\nTraining completed. Best {MONITOR}: {early_stopper.best_score}. Best epoch: {early_stopper.best_epoch+1 if early_stopper.best_epoch is not None else 'N/A'}\")\n",
    "    print(\"Checkpoints in\", OUTPUT_DIR, \"and full .h5 models in\", ARCHIVE_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
