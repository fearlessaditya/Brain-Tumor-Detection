{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f486bb6f-a580-40bf-9134-3f7459433adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Loading model from: outputs/best_model.h5\n",
      "üéØ Class names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\Downloads\\fakeimgenv311\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shiva\\Downloads\\fakeimgenv311\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_9660\\4128291618.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checkpoint type: <class 'collections.OrderedDict'>\n",
      "üîç Checkpoint keys: odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked', 'fc.1.weight', 'fc.1.bias', 'fc.3.weight', 'fc.3.bias', 'fc.3.running_mean', 'fc.3.running_var', 'fc.3.num_batches_tracked', 'fc.5.weight', 'fc.5.bias'])\n",
      "‚úÖ Model loaded successfully from outputs/best_model.h5\n",
      "‚úÖ Model and classes loaded successfully\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\Downloads\\fakeimgenv311\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# brain_tumor_gui.py\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, \n",
    "                             QHBoxLayout, QPushButton, QLabel, QFileDialog, \n",
    "                             QTextEdit, QGroupBox, QProgressBar, QMessageBox)\n",
    "from PyQt5.QtCore import Qt, QThread, pyqtSignal\n",
    "from PyQt5.QtGui import QPixmap, QFont\n",
    "\n",
    "class BrainTumorModel:\n",
    "    def __init__(self, model_path, class_names):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.class_names = class_names\n",
    "        self.model = self.load_model(model_path)\n",
    "        self.transform = self.get_transform()\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Load the trained model with correct architecture\"\"\"\n",
    "        try:\n",
    "            # Create the same model architecture as during training\n",
    "            model = models.resnet50(pretrained=False)\n",
    "            \n",
    "            # Use the EXACT same classifier structure as training\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(in_features, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, len(self.class_names))\n",
    "            )\n",
    "            \n",
    "            # Load weights - handle different file formats\n",
    "            if torch.cuda.is_available():\n",
    "                checkpoint = torch.load(model_path)\n",
    "            else:\n",
    "                checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "            \n",
    "            print(\"üîç Checkpoint type:\", type(checkpoint))\n",
    "            print(\"üîç Checkpoint keys:\", checkpoint.keys() if isinstance(checkpoint, dict) else \"Not a dict\")\n",
    "            \n",
    "            # Handle different checkpoint formats\n",
    "            if isinstance(checkpoint, dict):\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                else:\n",
    "                    model.load_state_dict(checkpoint)\n",
    "            else:\n",
    "                # Direct state dict\n",
    "                model.load_state_dict(checkpoint)\n",
    "                \n",
    "            model = model.to(self.device)\n",
    "            model.eval()\n",
    "            print(\"‚úÖ Model loaded successfully from\", model_path)\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            # Try alternative loading method\n",
    "            return self.load_model_alternative(model_path)\n",
    "    \n",
    "    def load_model_alternative(self, model_path):\n",
    "        \"\"\"Alternative loading method for different formats\"\"\"\n",
    "        try:\n",
    "            model = models.resnet50(pretrained=False)\n",
    "            in_features = model.fc.in_features\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(in_features, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, len(self.class_names))\n",
    "            )\n",
    "            \n",
    "            # Load state dict with strict=False to ignore minor mismatches\n",
    "            if torch.cuda.is_available():\n",
    "                checkpoint = torch.load(model_path)\n",
    "            else:\n",
    "                checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "            \n",
    "            if isinstance(checkpoint, dict):\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    state_dict = checkpoint['model_state_dict']\n",
    "                else:\n",
    "                    state_dict = checkpoint\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "                \n",
    "            # Filter out unexpected keys\n",
    "            model_state_dict = model.state_dict()\n",
    "            filtered_state_dict = {k: v for k, v in state_dict.items() if k in model_state_dict}\n",
    "            \n",
    "            model.load_state_dict(filtered_state_dict, strict=False)\n",
    "            model = model.to(self.device)\n",
    "            model.eval()\n",
    "            print(\"‚úÖ Model loaded with alternative method\")\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Alternative loading failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_transform(self):\n",
    "        \"\"\"Get the same transform used during training\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Predict the class of the brain MRI image\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = self.transform(image)\n",
    "            input_batch = input_tensor.unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_batch)\n",
    "                probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 0)\n",
    "                \n",
    "            predicted_class = self.class_names[predicted_idx.item()]\n",
    "            confidence_percent = confidence.item() * 100\n",
    "            \n",
    "            return predicted_class, confidence_percent, probabilities.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Prediction error: {e}\")\n",
    "            return None, 0, None\n",
    "\n",
    "class PredictionThread(QThread):\n",
    "    prediction_finished = pyqtSignal(str, float, list, str)\n",
    "    prediction_error = pyqtSignal(str)\n",
    "    \n",
    "    def __init__(self, model, image_path):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.image_path = image_path\n",
    "    \n",
    "    def run(self):\n",
    "        try:\n",
    "            predicted_class, confidence, probabilities = self.model.predict(self.image_path)\n",
    "            if predicted_class:\n",
    "                self.prediction_finished.emit(\n",
    "                    predicted_class, confidence, probabilities.tolist() if probabilities is not None else [], \n",
    "                    self.image_path\n",
    "                )\n",
    "            else:\n",
    "                self.prediction_error.emit(\"Prediction failed\")\n",
    "        except Exception as e:\n",
    "            self.prediction_error.emit(str(e))\n",
    "\n",
    "class BrainTumorGUI(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "        self.current_image_path = None\n",
    "        self.init_ui()\n",
    "        self.load_model()\n",
    "        \n",
    "    def init_ui(self):\n",
    "        \"\"\"Initialize the user interface\"\"\"\n",
    "        self.setWindowTitle(\"üß† Brain Tumor Classification System\")\n",
    "        self.setFixedSize(1000, 700)\n",
    "        \n",
    "        # Set dark theme\n",
    "        self.set_dark_theme()\n",
    "        \n",
    "        # Central widget\n",
    "        central_widget = QWidget()\n",
    "        self.setCentralWidget(central_widget)\n",
    "        \n",
    "        # Main layout\n",
    "        layout = QHBoxLayout()\n",
    "        central_widget.setLayout(layout)\n",
    "        \n",
    "        # Left panel - Image display\n",
    "        left_panel = self.create_image_panel()\n",
    "        layout.addWidget(left_panel, 2)\n",
    "        \n",
    "        # Right panel - Controls and results\n",
    "        right_panel = self.create_control_panel()\n",
    "        layout.addWidget(right_panel, 1)\n",
    "        \n",
    "    def set_dark_theme(self):\n",
    "        \"\"\"Set dark theme for the application\"\"\"\n",
    "        self.setStyleSheet(\"\"\"\n",
    "            QMainWindow {\n",
    "                background-color: #2b2b2b;\n",
    "                color: #ffffff;\n",
    "            }\n",
    "            QGroupBox {\n",
    "                color: #ffffff;\n",
    "                font-weight: bold;\n",
    "                border: 2px solid #555555;\n",
    "                border-radius: 5px;\n",
    "                margin-top: 1ex;\n",
    "                padding-top: 10px;\n",
    "            }\n",
    "            QGroupBox::title {\n",
    "                subcontrol-origin: margin;\n",
    "                left: 10px;\n",
    "                padding: 0 5px 0 5px;\n",
    "            }\n",
    "            QPushButton {\n",
    "                background-color: #4CAF50;\n",
    "                border: none;\n",
    "                color: white;\n",
    "                padding: 10px;\n",
    "                text-align: center;\n",
    "                text-decoration: none;\n",
    "                font-size: 14px;\n",
    "                margin: 4px 2px;\n",
    "                border-radius: 5px;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            QPushButton:hover {\n",
    "                background-color: #45a049;\n",
    "            }\n",
    "            QPushButton:pressed {\n",
    "                background-color: #3d8b40;\n",
    "            }\n",
    "            QPushButton:disabled {\n",
    "                background-color: #666666;\n",
    "                color: #999999;\n",
    "            }\n",
    "            QLabel {\n",
    "                color: #ffffff;\n",
    "            }\n",
    "            QTextEdit {\n",
    "                background-color: #1e1e1e;\n",
    "                color: #ffffff;\n",
    "                border: 1px solid #555555;\n",
    "                border-radius: 3px;\n",
    "                padding: 5px;\n",
    "            }\n",
    "            QProgressBar {\n",
    "                border: 2px solid #555555;\n",
    "                border-radius: 5px;\n",
    "                text-align: center;\n",
    "                color: white;\n",
    "                background-color: #1e1e1e;\n",
    "            }\n",
    "            QProgressBar::chunk {\n",
    "                background-color: #4CAF50;\n",
    "                width: 20px;\n",
    "            }\n",
    "        \"\"\")\n",
    "    \n",
    "    def create_image_panel(self):\n",
    "        \"\"\"Create the image display panel\"\"\"\n",
    "        panel = QGroupBox(\"Brain MRI Scan\")\n",
    "        layout = QVBoxLayout()\n",
    "        \n",
    "        # Image display label\n",
    "        self.image_label = QLabel()\n",
    "        self.image_label.setAlignment(Qt.AlignCenter)\n",
    "        self.image_label.setMinimumSize(600, 500)\n",
    "        self.image_label.setStyleSheet(\"background-color: #1e1e1e; border: 2px dashed #555555;\")\n",
    "        self.image_label.setText(\"No image selected\\n\\nClick 'Load MRI Scan' to begin\")\n",
    "        self.image_label.setFont(QFont(\"Arial\", 12))\n",
    "        \n",
    "        layout.addWidget(self.image_label)\n",
    "        panel.setLayout(layout)\n",
    "        return panel\n",
    "    \n",
    "    def create_control_panel(self):\n",
    "        \"\"\"Create the control and results panel\"\"\"\n",
    "        panel = QGroupBox(\"Analysis Controls\")\n",
    "        layout = QVBoxLayout()\n",
    "        \n",
    "        # Load image button\n",
    "        self.load_btn = QPushButton(\"üìÅ Load MRI Scan\")\n",
    "        self.load_btn.clicked.connect(self.load_image)\n",
    "        layout.addWidget(self.load_btn)\n",
    "        \n",
    "        # Analyze button\n",
    "        self.analyze_btn = QPushButton(\"üîç Analyze Tumor\")\n",
    "        self.analyze_btn.clicked.connect(self.analyze_image)\n",
    "        self.analyze_btn.setEnabled(False)\n",
    "        layout.addWidget(self.analyze_btn)\n",
    "        \n",
    "        # Progress bar\n",
    "        self.progress_bar = QProgressBar()\n",
    "        self.progress_bar.setVisible(False)\n",
    "        layout.addWidget(self.progress_bar)\n",
    "        \n",
    "        # Results section\n",
    "        results_group = QGroupBox(\"Analysis Results\")\n",
    "        results_layout = QVBoxLayout()\n",
    "        \n",
    "        self.results_text = QTextEdit()\n",
    "        self.results_text.setReadOnly(True)\n",
    "        self.results_text.setMaximumHeight(200)\n",
    "        self.results_text.setHtml(\"\"\"\n",
    "            <center>\n",
    "                <h3>üß† Brain Tumor Classifier</h3>\n",
    "                <p>Load a brain MRI scan to analyze for tumor detection</p>\n",
    "                <p><b>Model Status:</b> Loading...</p>\n",
    "            </center>\n",
    "        \"\"\")\n",
    "        results_layout.addWidget(self.results_text)\n",
    "        \n",
    "        # Confidence bars\n",
    "        self.confidence_group = QGroupBox(\"Confidence Levels\")\n",
    "        confidence_layout = QVBoxLayout()\n",
    "        \n",
    "        self.confidence_labels = {}\n",
    "        for i in range(4):\n",
    "            label = QLabel()\n",
    "            label.setVisible(False)\n",
    "            confidence_layout.addWidget(label)\n",
    "            self.confidence_labels[i] = label\n",
    "        \n",
    "        self.confidence_group.setLayout(confidence_layout)\n",
    "        results_layout.addWidget(self.confidence_group)\n",
    "        \n",
    "        results_group.setLayout(results_layout)\n",
    "        layout.addWidget(results_group)\n",
    "        \n",
    "        # Status label\n",
    "        self.status_label = QLabel(\"Loading model...\")\n",
    "        self.status_label.setStyleSheet(\"color: #888888; font-style: italic;\")\n",
    "        layout.addWidget(self.status_label)\n",
    "        \n",
    "        panel.setLayout(layout)\n",
    "        return panel\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model\"\"\"\n",
    "        try:\n",
    "            # Try different model file formats\n",
    "            model_paths = [\n",
    "                \"outputs/best_model.h5\",\n",
    "                \"outputs/best_model.keras\", \n",
    "                \"outputs/best_model.legacy\",\n",
    "                \"outputs/final_model.h5\",\n",
    "                \"outputs/final_model.keras\",\n",
    "                \"outputs/final_model.legacy\"\n",
    "            ]\n",
    "            \n",
    "            model_path = None\n",
    "            for path in model_paths:\n",
    "                if os.path.exists(path):\n",
    "                    model_path = path\n",
    "                    break\n",
    "            \n",
    "            if not model_path:\n",
    "                QMessageBox.critical(self, \"Error\", \n",
    "                    \"No model file found! Please ensure model files are in the 'outputs' folder.\")\n",
    "                return\n",
    "            \n",
    "            # Load class names\n",
    "            class_info_path = \"outputs/class_info.json\"\n",
    "            if os.path.exists(class_info_path):\n",
    "                with open(class_info_path, 'r') as f:\n",
    "                    class_info = json.load(f)\n",
    "                class_names = class_info.get('classes', ['Glioma', 'Meningioma', 'Pituitary', 'No Tumor'])\n",
    "            else:\n",
    "                # Try training_info.json\n",
    "                training_info_path = \"outputs/training_info.json\"\n",
    "                if os.path.exists(training_info_path):\n",
    "                    with open(training_info_path, 'r') as f:\n",
    "                        training_info = json.load(f)\n",
    "                    class_names = training_info.get('classes', ['Glioma', 'Meningioma', 'Pituitary', 'No Tumor'])\n",
    "                else:\n",
    "                    class_names = ['Glioma', 'Meningioma', 'Pituitary', 'No Tumor']\n",
    "            \n",
    "            print(f\"üéØ Loading model from: {model_path}\")\n",
    "            print(f\"üéØ Class names: {class_names}\")\n",
    "            \n",
    "            self.model = BrainTumorModel(model_path, class_names)\n",
    "            if self.model.model is None:\n",
    "                QMessageBox.critical(self, \"Error\", \"Failed to load the model!\")\n",
    "                self.status_label.setText(\"‚ùå Model loading failed\")\n",
    "                return\n",
    "                \n",
    "            self.status_label.setText(\"‚úÖ Model loaded successfully\")\n",
    "            self.results_text.setHtml(f\"\"\"\n",
    "                <center>\n",
    "                    <h3>üß† Brain Tumor Classifier</h3>\n",
    "                    <p>Model loaded successfully!</p>\n",
    "                    <p><b>Supported classes:</b><br>\n",
    "                    - {class_names[0]}<br>\n",
    "                    - {class_names[1]}<br>\n",
    "                    - {class_names[2]}<br>\n",
    "                    - {class_names[3]}</p>\n",
    "                </center>\n",
    "            \"\"\")\n",
    "            print(\"‚úÖ Model and classes loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            QMessageBox.critical(self, \"Error\", f\"Failed to load model: {str(e)}\")\n",
    "            self.status_label.setText(\"‚ùå Model loading failed\")\n",
    "    \n",
    "    def load_image(self):\n",
    "        \"\"\"Load an image for analysis\"\"\"\n",
    "        file_path, _ = QFileDialog.getOpenFileName(\n",
    "            self, \"Select Brain MRI Scan\", \"\",\n",
    "            \"Image Files (*.png *.jpg *.jpeg *.bmp *.tiff)\"\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            self.current_image_path = file_path\n",
    "            pixmap = QPixmap(file_path)\n",
    "            \n",
    "            # Scale image to fit label while maintaining aspect ratio\n",
    "            scaled_pixmap = pixmap.scaled(\n",
    "                self.image_label.width() - 20, \n",
    "                self.image_label.height() - 20,\n",
    "                Qt.KeepAspectRatio, \n",
    "                Qt.SmoothTransformation\n",
    "            )\n",
    "            \n",
    "            self.image_label.setPixmap(scaled_pixmap)\n",
    "            self.analyze_btn.setEnabled(True)\n",
    "            self.status_label.setText(f\"Loaded: {Path(file_path).name}\")\n",
    "            \n",
    "            # Clear previous results\n",
    "            self.results_text.clear()\n",
    "            for label in self.confidence_labels.values():\n",
    "                label.setVisible(False)\n",
    "    \n",
    "    def analyze_image(self):\n",
    "        \"\"\"Analyze the loaded image\"\"\"\n",
    "        if not self.current_image_path or not self.model:\n",
    "            return\n",
    "        \n",
    "        self.analyze_btn.setEnabled(False)\n",
    "        self.progress_bar.setVisible(True)\n",
    "        self.progress_bar.setRange(0, 0)  # Indeterminate progress\n",
    "        \n",
    "        # Start prediction in separate thread\n",
    "        self.prediction_thread = PredictionThread(self.model, self.current_image_path)\n",
    "        self.prediction_thread.prediction_finished.connect(self.on_prediction_finished)\n",
    "        self.prediction_thread.prediction_error.connect(self.on_prediction_error)\n",
    "        self.prediction_thread.start()\n",
    "    \n",
    "    def on_prediction_finished(self, predicted_class, confidence, probabilities, image_path):\n",
    "        \"\"\"Handle prediction results\"\"\"\n",
    "        self.progress_bar.setVisible(False)\n",
    "        self.analyze_btn.setEnabled(True)\n",
    "        \n",
    "        # Determine color based on prediction\n",
    "        if \"No Tumor\" in predicted_class:\n",
    "            color = \"green\"\n",
    "            emoji = \"‚úÖ\"\n",
    "            status = \"Healthy\"\n",
    "        else:\n",
    "            color = \"red\" \n",
    "            emoji = \"‚ö†Ô∏è\"\n",
    "            status = \"Medical Attention Required\"\n",
    "        \n",
    "        # Display results\n",
    "        result_html = f\"\"\"\n",
    "            <center>\n",
    "                <h2 style=\"color: {color};\">{emoji} {predicted_class}</h2>\n",
    "                <h3 style=\"color: {color};\">Confidence: {confidence:.2f}%</h3>\n",
    "                <p><b>Status:</b> {status}</p>\n",
    "            </center>\n",
    "        \"\"\"\n",
    "        \n",
    "        self.results_text.setHtml(result_html)\n",
    "        self.status_label.setText(f\"Analysis complete - {predicted_class}\")\n",
    "        \n",
    "        # Show confidence bars for all classes\n",
    "        class_names = self.model.class_names\n",
    "        for i, (class_name, prob) in enumerate(zip(class_names, probabilities)):\n",
    "            confidence_percent = prob * 100\n",
    "            bar_width = min(int(confidence_percent * 2), 200)  # Scale for visual bar\n",
    "            \n",
    "            # Color coding\n",
    "            if class_name == predicted_class:\n",
    "                bar_color = \"#4CAF50\" if \"No Tumor\" in class_name else \"#ff4444\"\n",
    "            else:\n",
    "                bar_color = \"#666666\"\n",
    "            \n",
    "            bar_html = f\"\"\"\n",
    "                <div style=\"margin: 5px 0;\">\n",
    "                    <div style=\"display: flex; justify-content: space-between;\">\n",
    "                        <span>{class_name}</span>\n",
    "                        <span>{confidence_percent:.1f}%</span>\n",
    "                    </div>\n",
    "                    <div style=\"background: #333; border-radius: 3px; height: 20px;\">\n",
    "                        <div style=\"background: {bar_color}; width: {bar_width}px; height: 100%; border-radius: 3px;\"></div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            self.confidence_labels[i].setText(bar_html)\n",
    "            self.confidence_labels[i].setVisible(True)\n",
    "        \n",
    "        # Show medical advice\n",
    "        if \"No Tumor\" not in predicted_class:\n",
    "            self.show_medical_advice(predicted_class)\n",
    "    \n",
    "    def on_prediction_error(self, error_message):\n",
    "        \"\"\"Handle prediction errors\"\"\"\n",
    "        self.progress_bar.setVisible(False)\n",
    "        self.analyze_btn.setEnabled(True)\n",
    "        QMessageBox.critical(self, \"Analysis Error\", f\"Failed to analyze image:\\n{error_message}\")\n",
    "        self.status_label.setText(\"Analysis failed\")\n",
    "    \n",
    "    def show_medical_advice(self, tumor_type):\n",
    "        \"\"\"Show medical advice for detected tumors\"\"\"\n",
    "        advice_map = {\n",
    "            \"Glioma\": \"Gliomas are tumors that occur in the brain and spinal cord. Consult a neurologist immediately for further evaluation and treatment planning.\",\n",
    "            \"Meningioma\": \"Meningiomas are tumors that arise from the meninges. While often benign, they should be evaluated by a neurosurgeon for potential treatment.\",\n",
    "            \"Pituitary\": \"Pituitary tumors affect the pituitary gland at the base of the brain. Endocrine evaluation and neurosurgical consultation are recommended.\"\n",
    "        }\n",
    "        \n",
    "        advice = advice_map.get(tumor_type, \"Please consult with a medical professional for proper diagnosis and treatment.\")\n",
    "        \n",
    "        self.results_text.append(f\"\"\"\n",
    "            <br>\n",
    "            <div style=\"background: #ff4444; padding: 10px; border-radius: 5px; color: white;\">\n",
    "                <b>‚ö†Ô∏è Medical Recommendation:</b><br>\n",
    "                {advice}\n",
    "            </div>\n",
    "        \"\"\")\n",
    "\n",
    "def main():\n",
    "    app = QApplication(sys.argv)\n",
    "    \n",
    "    # Set application properties\n",
    "    app.setApplicationName(\"Brain Tumor Classification System\")\n",
    "    app.setApplicationVersion(\"1.0\")\n",
    "    \n",
    "    # Create and show main window\n",
    "    window = BrainTumorGUI()\n",
    "    window.show()\n",
    "    \n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e80f60-bc78-4cf7-a05e-d858a6913bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FakeImageEnv311",
   "language": "python",
   "name": "fakeimgenv311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
